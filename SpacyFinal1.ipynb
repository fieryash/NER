{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xx8mdA6g0SyD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "N5yRHmSWwCuk",
    "outputId": "da64884d-5e16-4aab-8f4a-ae3ae0003500"
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P8UpTUPIv0iz",
    "outputId": "da9fb4fb-077b-45be-b736-bdceeef0c05b"
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import string \n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "\n",
    "#Spacy imports\n",
    "import spacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.util import decaying\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "import plac\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-s1Ahwlv8TD"
   },
   "outputs": [],
   "source": [
    "#reading all json files\n",
    "lst = []\n",
    "file = r\"E:\\Lawnics\\Spacy Acts and Cases\\output1.jsonl\"\n",
    "with jsonlines.open(file) as reader:\n",
    "    for obj in reader:\n",
    "        lst.append(obj)\n",
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7UjYhL2Jwuzk",
    "outputId": "6e69650f-4cef-4341-91d2-46b7a67b7a1a"
   },
   "outputs": [],
   "source": [
    "#creating the dataset (which will be cleaned in further steps)\n",
    "Dataset = []\n",
    "for data in lst:\n",
    "    ents = [tuple(entity) for entity in data['labels']]\n",
    "    Dataset.append((data['text'].lower().strip(),{'entities':ents}))\n",
    "random.shuffle(Dataset)\n",
    "Dataset = Dataset[0:2000]\n",
    "test_dataset = Dataset[1500:2000]\n",
    "print(len(Dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def punc(text):\n",
    "#     # define punctuation\n",
    "#     punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "#     my_str = text\n",
    "\n",
    "#     # To take input from the user\n",
    "#     # my_str = input(\"Enter a string: \")\n",
    "\n",
    "#     # remove punctuation from the string\n",
    "#     no_punct = \"\"\n",
    "#     for char in my_str:\n",
    "#         if char not in punctuations:\n",
    "#             no_punct = no_punct + char\n",
    "\n",
    "#     # display the unpunctuated string\n",
    "#     return no_punct\n",
    "# new_lst = []\n",
    "# for item in Dataset:\n",
    "#     new_lst.append([item[0],item[1]])\n",
    "# for i in tqdm(new_lst):\n",
    "#     i[0] = punc(i[0])\n",
    "# new_lst[0]\n",
    "# Dataset = []\n",
    "# for j in new_lst:\n",
    "#     Dataset.append((j[0],j[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ojr0faHnwyhv"
   },
   "outputs": [],
   "source": [
    "#creating a dictionary of the annotations\n",
    "def create_dict(x):\n",
    "    dictionary = {}\n",
    "    text = x[0]\n",
    "    annot = x[1]\n",
    "    index = annot['entities']\n",
    "    for start,end,tag in index:\n",
    "        word = text[start:end].lower().strip()\n",
    "        dictionary[word]=tag\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "odLyfUQawzsc",
    "outputId": "682661b4-98a2-4d0f-8b1b-b58f8e96f9b0"
   },
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for val in Dataset:\n",
    "    tmp_dict = create_dict(val)\n",
    "#     print(tmp_dict)\n",
    "#     break\n",
    "    dictionary.update(tmp_dict)\n",
    "    \n",
    "    \n",
    "dictionary = {k.strip().lower(): v for (k, v) in dictionary.items()}\n",
    "len(dictionary)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elHfoiEIw0tI"
   },
   "outputs": [],
   "source": [
    "for keys in dictionary.copy():\n",
    "    if len(keys) <= 4:\n",
    "        dictionary.pop(keys)\n",
    "    if \"act\" in keys:\n",
    "        dictionary[keys] = \"Act\"\n",
    "    if \"vs\" in keys:\n",
    "        dictionary[keys] = \"Cases\"\n",
    "dictionary[\"ipc\"] = \"Act\"\n",
    "dictionary[\"indian penal code\"] = \"Act\"\n",
    "dictionary[\"code of criminal procedure\"] = \"Act\"\n",
    "dictionary[\"code of civil procedure\"] = \"Act\"\n",
    "dictionary[\"the criminal procedure code\"] = \"Act\"\n",
    "dictionary[\"cr.pc\"] = \"Act\"\n",
    "dictionary[\"cr.p.c\"] = \"Act\"\n",
    "# dictionary.pop(\"free for one month\")\n",
    "for keys in dictionary.copy():\n",
    "    if dictionary[keys] == \"Other\":\n",
    "        dictionary.pop(keys)\n",
    "\n",
    "for k in dictionary.copy():\n",
    "    if \"act\" in k:\n",
    "        dictionary[k] = \"Act\"\n",
    "#     if dictionary[k] == \"Cases\":\n",
    "#         res= re.split(r'(^[^\\d]+)', k)\n",
    "#         if len(res) > 1:\n",
    "#             res = res[1]\n",
    "#             dictionary[res] = \"Cases\"\n",
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPWQ9z-s5pqW"
   },
   "outputs": [],
   "source": [
    "# Dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cUq5IqhEEqH"
   },
   "outputs": [],
   "source": [
    "#Checking if overlapping annotations exist. For example \"Judgement and udgement(wrongly annotated)\" or \"Supreme Court and\"\n",
    "def is_overlap(a, b):\n",
    "    a, b = sorted([a, b])\n",
    "    if a[0] <= b[0] and a[1] >= b[1]:\n",
    "        return True\n",
    "    elif a[1] > b[0]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qs0c9BEJD2q6"
   },
   "outputs": [],
   "source": [
    "# assert is_overlap((1,9), (3,4)) == True\n",
    "# assert is_overlap((3,4), (1,9)) == True\n",
    "# assert is_overlap((1, 5), (3, 7)) == True\n",
    "# assert is_overlap((4, 9), (1, 6)) == True\n",
    "# assert is_overlap((1,6), (7,9)) == False\n",
    "# assert is_overlap((1,4),(4,6)) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JH-xC4HQEw_N"
   },
   "outputs": [],
   "source": [
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8pZT_MmEpHz"
   },
   "outputs": [],
   "source": [
    "# [k for k in dictionary if len(k) == 44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHbSlbJvEfbG"
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# Counter(map(len, dictionary)).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITJo3y_2HcjD"
   },
   "outputs": [],
   "source": [
    "# sorted(dictionary.items(), key=lambda x: len(x[0]), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xT8JOzgtM7n9"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUIYydQ5M7n_"
   },
   "outputs": [],
   "source": [
    "#Script to clean out the annotations\n",
    "dictionary2 = {re.compile(re.escape(k)): v for k,v in sorted(dictionary.items(), key=lambda x: len(x[0]), reverse=True) if len(k) >= 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIyLweHsw3Dn"
   },
   "outputs": [],
   "source": [
    "#Script to re-annotate the dataset\n",
    "from __future__ import unicode_literals, print_function\n",
    "def entities_finder(document):  \n",
    "    entities = []\n",
    "    my_str=document\n",
    "    some_list= set()\n",
    "    for key, final_tag in dictionary2.items():\n",
    "        start = 0\n",
    "        for match in key.finditer(my_str):\n",
    "            out_interval = match.span()\n",
    "            if out_interval in some_list:\n",
    "                continue\n",
    "            if not any(is_overlap(out_interval, interval) for interval in some_list):\n",
    "                entities.append((out_interval[0], out_interval[1], final_tag))\n",
    "                some_list.add(out_interval)\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkioIN6fxA9y"
   },
   "outputs": [],
   "source": [
    "#To store in the format specified for Spacy\n",
    "def formatting_func(text):\n",
    "    val=(text,{'entities':entities_finder(text)})\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "aW3QXfOaxDDo",
    "outputId": "4f643a3b-4f51-4de5-8fe0-124fac49a0e0"
   },
   "outputs": [],
   "source": [
    "cleaned_dataset=[]\n",
    "for i in tqdm(sorted(Dataset, key=lambda x: len(x[0]))):\n",
    "    cleaned_dataset.append(formatting_func(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for item in tqdm(cleaned_dataset):\n",
    "    lst = []\n",
    "    for ent in item[1][\"entities\"]:\n",
    "        if ent[2] != \"Cases\":\n",
    "            lst.append(ent)\n",
    "        if ent[2] == \"Cases\":\n",
    "            st = item[0][ent[0]:ent[1]] \n",
    "            if st != \"others\":\n",
    "                if \"v\" in st or \"vs\" in st:\n",
    "                    if \"penal\" not in st:\n",
    "                        lst.append(ent)\n",
    "    item[1][\"entities\"] = lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in item[1][\"entities\"]:\n",
    "        if ent[2] == \"Cases\":\n",
    "            st = item[0][ent[0]:ent[1]] \n",
    "            print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(cleaned_dataset):\n",
    "    for n in item[1][\"entities\"]:\n",
    "        if n[2] == \"Other\":\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJl3Wi-hM7oI"
   },
   "outputs": [],
   "source": [
    "# len(sorted(Dataset, key=lambda x: len(x[0]),reverse=True)[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-DCWJEhM7oK"
   },
   "outputs": [],
   "source": [
    "# len(sorted(Dataset, key=lambda x: len(x[0]),reverse=True)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PhYLReFM7oM"
   },
   "outputs": [],
   "source": [
    "cleaned_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9FxGVzhxV9u"
   },
   "outputs": [],
   "source": [
    "random.shuffle(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7BQe3ONzxJN5",
    "outputId": "71f58cce-c990-4ea5-b47b-85a9323a33e5"
   },
   "outputs": [],
   "source": [
    "train_set = cleaned_dataset[:200]\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U9a09xtjxSyc",
    "outputId": "e81c4e9e-b5a5-4695-cc0e-35512e6fee96"
   },
   "outputs": [],
   "source": [
    "test_set = cleaned_dataset[500:1000]\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYsmGKFS660z"
   },
   "outputs": [],
   "source": [
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot['entities'])\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "id": "L3rQ26a7xawN",
    "outputId": "8daa5dd1-4122-4f82-bbe5-4030aea557b1"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\n",
    "# Training additional entity types using spaCy\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "# LABEL = ['Authority', 'Action', 'Area', 'Party', 'Subject', 'SubjectElements'] #currently used annotations\n",
    "LABEL = ['Section', 'Article', 'Cases', 'Act'] #currently used annotations\n",
    "def main(model=None, new_model_name='new_model', output_dir=r\"E:\\Lawnics\\Spacy Acts and Cases\", n_iter=5):\n",
    "    \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
    "    max_batch_size = 16\n",
    "    if len(train_set) < 500:\n",
    "        max_batch_size /= 2\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "        nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)   # Add new entity labels to entity recognizer\n",
    "\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_set)\n",
    "            losses = {}T\n",
    "            batches = minibatch(train_set, size=compounding(1, max_batch_size, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.5,\n",
    "                           losses=losses)\n",
    "            scores = evaluate(nlp, train_set)\n",
    "            print(\"Scores \",scores)\n",
    "            print('Losses', losses)\n",
    "                # Save model \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "            \n",
    "prdnlp = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p_g6G8RExd5S",
    "outputId": "1c5d9dac-e8fa-4ad9-cbda-88f004126b30"
   },
   "outputs": [],
   "source": [
    "# to render the generated annotations\n",
    "nlp2 = spacy.load(r\"E:\\Lawnics\\Spacy Acts and Cases\")\n",
    "doc = nlp2(test_set[100][0])\n",
    "displacy.render(nlp2(str(doc)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.gold import docs_to_json\n",
    "doc = nlp2(test_dataset[150][0])\n",
    "json_data = docs_to_json([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nkAMQQrvxm46",
    "outputId": "1c914d93-6f04-4acf-ec67-7c0d0f186221"
   },
   "outputs": [],
   "source": [
    "#Evaluate the model on the test_set created after auto-annotations\n",
    "examples = test_set[10:50]\n",
    "results = evaluate(nlp2, examples)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9Ic4OsQ5an3"
   },
   "outputs": [],
   "source": [
    "# This checks for mis-alignments in the annotations shown by ('-'). Spacy ignores these as a Userwarning\n",
    "# But too many mis-alignments will lead to a crunch in your train set\n",
    "\n",
    "from spacy.gold import biluo_tags_from_offsets\n",
    "for text, annot in train_set:\n",
    "    doc = nlp2.make_doc(text)\n",
    "    print(biluo_tags_from_offsets(doc, annot[\"entities\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# from blackstone.displacy_palette import ner_displacy_options\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# text = \"\"\"\n",
    "# The applicant must satisfy a high standard. This is a case where the action is to be tried by a judge with a jury. The standard is set out in Jameel v Wall Street Journal Europe Sprl [2004] EMLR 89, para 14:\n",
    "# “But every time a meaning is shut out (including any holding that the words complained of either are, or are not, capable of bearing a defamatory meaning) it must be remembered that the judge is taking it upon himself to rule in effect that any jury would be perverse to take a different view on the question. It is a high threshold of exclusion. Ever since Fox’s Act 1792 (32 Geo 3, c 60) the meaning of words in civil as well as criminal libel proceedings has been constitutionally a matter for the jury. The judge’s function is no more and no less than to pre-empt perversity. That being clearly the position with regard to whether or not words are capable of being understood as defamatory or, as the case may be, non-defamatory, I see no basis on which it could sensibly be otherwise with regard to differing levels of defamatory meaning. Often the question whether words are defamatory at all and, if so, what level of defamatory meaning they bear will overlap.”\n",
    "# 18 In Berezovsky v Forbes Inc [2001] EMLR 1030, para 16 Sedley LJ had stated the test this way:\n",
    "# “The real question in the present case is how the courts ought to go about ascertaining the range of legitimate meanings. Eady J regarded it as a matter of impression. That is all right, it seems to us, provided that the impression is not of what the words mean but of what a jury could sensibly think they meant. Such an exercise is an exercise in generosity, not in parsimony.”\n",
    "# \"\"\"\n",
    "text = Dataset[100][0]\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Call displacy and pass `ner_displacy_options` into the option parameter`\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "# load english language model\n",
    "nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])\n",
    "\n",
    "# text = \"This is a sample sentence.\"\n",
    "\n",
    "# create spacy \n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "#     print(token.text,'->',token.pos_)\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"asdkjah other\"\n",
    "lst = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "res = re.split(r'(^[^\\d]+)', test_str)\n",
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\"l\":\"other\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dct:\n",
    "    if dct[k] == \"other\":\n",
    "        dct[k] = \"Cases\"\n",
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SpacyFinal_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
